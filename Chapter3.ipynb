{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Spark_CLOB_Split\") \\\n",
    ".config(\"hive.metastore.sasl.enabled\", \"true\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 sparkContext,这个环境被包含在了 SparkSession 之中。\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.创建RDD，json数据\n",
    "stringJsonRDD = sc.parallelize((\"\"\"{\"id\":1,\"name\":\"A\",\"age\":19,\"eyeColor\":\"brown\"}\"\"\",\n",
    "                               \"\"\"{\"id\":2,\"name\":\"B\",\"age\":21,\"eyeColor\":\"blue\"}\"\"\",\n",
    "                               \"\"\"{\"id\":3,\"name\":\"C\",\"age\":18,\"eyeColor\":\"green\"}\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.利用spark.read.json()的方法将其转换为DataFrame\n",
    "swimmersJSON = spark.read.json(stringJsonRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.创建一个临时表\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----+\n",
      "|age|eyeColor| id|name|\n",
      "+---+--------+---+----+\n",
      "| 19|   brown|  1|   A|\n",
      "| 21|    blue|  2|   B|\n",
      "| 18|   green|  3|   C|\n",
      "+---+--------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4.使用DataFrame API来查询 show()\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id=1, name='A'),\n",
       " Row(age=21, eyeColor='blue', id=2, name='B'),\n",
       " Row(age=18, eyeColor='green', id=3, name='C')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.对DataFrame的SQL查询\n",
    "spark.sql(\"select * from swimmersJSON\").collect()\n",
    "#spark.sql(\"select * from swimmersJSON\").take(2)\n",
    "#spark.sql(\"select * from swimmersJSON\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6.编程指定模式\n",
    "#1）打印数据模式\n",
    "swimmersJSON.printSchema() #age:long type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2）指定模式\n",
    "from pyspark.sql.types import *\n",
    "stringCSVRDD = sc.parallelize([(1,\"A\",19,\"brown\"),(2,\"B\",21,\"blue\"),(3,\"C\",18,\"green\")])\n",
    "#设定模式\n",
    "schema = StructType([\n",
    "    StructField(\"id\",LongType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"age\",LongType(),True),\n",
    "    StructField(\"eyeColor\",StringType(),True) \n",
    "]) #字段名字，字段类型，字段是否为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+--------+\n",
      "| id|name|age|eyeColor|\n",
      "+---+----+---+--------+\n",
      "|  1|   A| 19|   brown|\n",
      "|  2|   B| 21|    blue|\n",
      "|  3|   C| 18|   green|\n",
      "+---+----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#对RDD应用该模式，并创建DataFrame:spark.createDataFrame(RDD,schema)\n",
    "swimmers = spark.createDataFrame(stringCSVRDD,schema)\n",
    "#利用DataFrame创建临时视图，才可以应用SQL查询\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")\n",
    "swimmers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.利用DataFrame API 查询\n",
    "# 得到DataFrame 行数\n",
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  2| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#运行筛选语句，使用filter筛选语句\n",
    "#获取age=21的id\n",
    "swimmers.select(\"id\",\"age\").filter(\"age=21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  2| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#另一种写法\n",
    "swimmers.select(swimmers.id,swimmers.age).filter(swimmers.age == 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|eyeColor|\n",
      "+----+--------+\n",
      "|   A|   brown|\n",
      "|   B|    blue|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#若想获得眼睛颜色以b开头的运动员名字，可以使用类似于SQL语句的like，用法为 like 'b%'\n",
    "swimmers.select(\"name\",\"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7.对DataFrame利用SQL查询\n",
    "#注：DataFrame是可以访问的，因为我们对swimmers执行了createOrReplaceTempView\n",
    "\n",
    "#行数\n",
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  2| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#利用where子句运行筛选语句\n",
    "spark.sql(\"select id,age from swimmers where age=21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|eyeColor|\n",
      "+----+--------+\n",
      "|   A|   brown|\n",
      "|   B|    blue|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name,eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8.DataFrame 场景-实时飞行性能\n",
    "\n",
    "#获得数据集，并创建视图\n",
    "flightPerfFilePath = 'file:///home/yanglab/lianhaimiao/PySpark/spark_data/flight-data/departuredelays.csv'\n",
    "airpathsFilePath = 'file:///home/yanglab/lianhaimiao/PySpark/spark_data/flight-data/airport-codes-na.txt'\n",
    "\n",
    "airports = spark.read.csv(airpathsFilePath,header = 'true', inferSchema = 'true', sep = '\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "flightPerf = spark.read.csv(flightPerfFilePath,header = 'true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "#cache\n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightPerf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#通过 城市City 和 起飞代码origin 查询 华盛顿州WA 的 航班延误总数Delays\n",
    "#1)对某一组内的delay求和\n",
    "#2) 根据a.IATA = f.origin 连接两个表\n",
    "#3) 筛选WA的数据\n",
    "#4) 根据a.City, f.origin分组\n",
    "#5) 按照sum(f.delay)降序排序，没有desc默认为升序\n",
    "spark.sql(\"\"\"\n",
    "select a.City, f.origin, sum(f.delay) as Delays  \n",
    "       from FlightPerformance f\n",
    "       join airports a\n",
    "        on a.IATA = f.origin                    \n",
    "        where a.State = 'WA'                    \n",
    "        group by a.City, f.origin               \n",
    "        order by sum(f.delay) desc              \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|State|   Delays|\n",
      "+-----+---------+\n",
      "|   TX|1994943.0|\n",
      "|   CA|1891919.0|\n",
      "|   IL|1630792.0|\n",
      "|   FL|1531877.0|\n",
      "|   GA|1191014.0|\n",
      "|   CO| 963061.0|\n",
      "|   NY| 878929.0|\n",
      "|   NV| 474208.0|\n",
      "|   NJ| 452791.0|\n",
      "|   AZ| 401793.0|\n",
      "+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#分析美国大陆上的所有州\n",
    "spark.sql(\"\"\"\n",
    "select a.State, sum(f.delay) as Delays\n",
    "      from FlightPerformance f\n",
    "       join airports a\n",
    "        on a.IATA = f.origin  \n",
    "        where a.Country = 'USA'\n",
    "        group by a.State\n",
    "        order by sum(f.delay) desc \n",
    "\"\"\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
