{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Spark_CLOB_Split\") \\\n",
    ".config(\"hive.metastore.sasl.enabled\", \"true\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你看到的代码如果以 spark. 开头，你zhi需要用 sc. 开头就行了。\n",
    "# 我这里已经给你 build 好了 sprak 的 context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 sparkContext,这个环境被包含在了 SparkSession 之中。\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1.创建 RDD，支持 tuple,dict,list类型\n",
    "data = sc.parallelize([\n",
    "    ('A','a'),\n",
    "    {'B':10000},\n",
    "    ['c','e','f']\n",
    "]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果对数据集使用方法.collect()，将RDD的所有元素返回给驱动程序，可以访问对象中的数据\n",
    "data[1]['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　/home/yanglab/lianhaimiao/PySpark/datasets　\n",
    "#　page_news.txt  VS14MORT.txt  VS14MORT.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                   1                                          2101  M1087 432311  4M4                2014U7CN                                    I64 238 070   24 0111I64                                                                                                                                                                           01 I64                                                                                                  01  11                                 100 601']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取外部文件\n",
    "#sc.textFile(路径,n) n表示数据集被划分的分区个数，一般分为２～４个\n",
    "#本地文件前加\"file://\"\n",
    "data_from_file = sc.textFile('file:///home/yanglab/lianhaimiao/PySpark/datasets/VS14MORT.txt.gz',4)\n",
    "#read data\n",
    "data_from_file.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这段代码用来解析文件，使其可读\n",
    "def extractInformation(row):\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    selected_indices = [\n",
    "         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,\n",
    "         19,21,22,23,24,25,27,28,29,30,32,33,34,\n",
    "         36,37,38,39,40,41,42,43,44,45,46,47,48,\n",
    "         49,50,51,52,53,54,55,56,58,60,61,62,63,\n",
    "         64,65,66,67,68,69,70,71,72,73,74,75,76,\n",
    "         77,78,79,81,82,83,84,85,87,89\n",
    "    ]\n",
    "    record_split = re\\\n",
    "        .compile(\n",
    "            r'([\\s]{19})([0-9]{1})([\\s]{40})([0-9\\s]{2})([0-9\\s]{1})([0-9]{1})([0-9]{2})' + \n",
    "            r'([\\s]{2})([FM]{1})([0-9]{1})([0-9]{3})([0-9\\s]{1})([0-9]{2})([0-9]{2})' + \n",
    "            r'([0-9]{2})([0-9\\s]{2})([0-9]{1})([SMWDU]{1})([0-9]{1})([\\s]{16})([0-9]{4})' +\n",
    "            r'([YNU]{1})([0-9\\s]{1})([BCOU]{1})([YNU]{1})([\\s]{34})([0-9\\s]{1})([0-9\\s]{1})' +\n",
    "            r'([A-Z0-9\\s]{4})([0-9]{3})([\\s]{1})([0-9\\s]{3})([0-9\\s]{3})([0-9\\s]{2})([\\s]{1})' + \n",
    "            r'([0-9\\s]{2})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([\\s]{36})([A-Z0-9\\s]{2})([\\s]{1})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([\\s]{1})([0-9\\s]{2})([0-9\\s]{1})' + \n",
    "            r'([0-9\\s]{1})([0-9\\s]{1})([0-9\\s]{1})([\\s]{33})([0-9\\s]{3})([0-9\\s]{1})([0-9\\s]{1})')\n",
    "    try:\n",
    "        rs = np.array(record_split.split(row))[selected_indices]\n",
    "    except:\n",
    "        rs = np.array(['-99'] * len(selected_indices))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file_conv = data_from_file.map(extractInformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['1', '  ', '2', '1', '01', 'M', '1', '087', ' ', '43', '23', '11',\n",
       "        '  ', '4', 'M', '4', '2014', 'U', '7', 'C', 'N', ' ', ' ', 'I64 ',\n",
       "        '238', '070', '   ', '24', '01', '11I64  ', '       ', '       ',\n",
       "        '       ', '       ', '       ', '       ', '       ', '       ',\n",
       "        '       ', '       ', '       ', '       ', '       ', '       ',\n",
       "        '       ', '       ', '       ', '       ', '       ', '01',\n",
       "        'I64  ', '     ', '     ', '     ', '     ', '     ', '     ',\n",
       "        '     ', '     ', '     ', '     ', '     ', '     ', '     ',\n",
       "        '     ', '     ', '     ', '     ', '     ', '     ', '01', ' ',\n",
       "        ' ', '1', '1', '100', '6'], dtype='<U40')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据\n",
    "data_from_file_conv.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）map()转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, -99]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个新的数据集，使死亡年份转换为数字\n",
    "data_2014 = data_from_file_conv.map(lambda row:int(row[16]))\n",
    "data_2014.take(10)   #take括号内的数字表示显示前几data_2014行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造元组\n",
    "data_2014_2 = data_from_file_conv.map(lambda row:(row[16],int(row[16])))\n",
    "data_2014_2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）filter()转换：从数据集中选择满足特征标准的数据，形成新的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_from_file_conv.filter(lambda row:row[16] == '2014' and row[21] == '0')\n",
    "data_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) flatMap()转换：和map()类似，但是返回的是一个扁平化结果，形成数组的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014', 2015, '2014', 2015, '2014']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014_flat =  data_from_file_conv.flatMap(lambda row:(row[16],int(row[16])+1))\n",
    "data_2014_flat.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) distict() 返回指定列中，不同值的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 'F', '-99']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distict_genter =  data_from_file_conv.map(lambda row:row[5]).distinct()\n",
    "distict_genter.collect() #.collect() show result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) sample()转换：随机采样， 参数定义：sample(采样是否应该替换，返回数据的分数值，伪随机数产生的种子)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.1\n",
    "data_sample = data_from_file_conv.sample(False,0.1,666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin data:2631171,sample data:263247\n"
     ]
    }
   ],
   "source": [
    "print('origin data:{},sample data:{}'.format(data_from_file_conv.count(),data_sample.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) .leftOuterJoin()转换：根据元组左侧都有的值，连接两个记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, '5')), ('a', (1, 1)), ('a', (1, 4)), ('c', (3, None))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "rdd2 = sc.parallelize([('a',1),('a',4),('b','5'),('d',6)])  \n",
    "rdd3 = rdd1.leftOuterJoin(rdd2)   #这里要注意的是最后结果中没有d，因为是rdd1.letfOuterJoin(rdd2)\n",
    "rdd3.collect()  #显示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, '5')), ('a', (1, 1)), ('a', (1, 4))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.join()\n",
    "rdd4 = rdd1.join(rdd2)\n",
    "rdd4.collect()   #注：这里只有a和b，没有存在none的c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intersection()：返回两个RDD中相等的元素\n",
    "rdd5 = rdd1.intersection(rdd2)\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) .reparttition():重新分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd1.repartition(4)\n",
    "len(rdd1.glom().collect())  #分区数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.take()\n",
    "data_first = data_from_file_conv.take(1)\n",
    "#.takeSample()\n",
    "data_first_sample = data_from_file_conv.takeSample(False,1,667) #三个参数，第二个参数表示随便采样返回几个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8）reduce():使用指定的方法减少RDD中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda row:row[1]).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 6), ('b', 7), ('a', 10), ('d', 3)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduceByKey():在键-键基础上减少RDD的元素\n",
    "data_key = sc.parallelize([('a',4),('a',6),('c',1),('d',3),('c',5),('b',7)],4)\n",
    "data_key.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) count():统计RDD中的元素数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('c', 2), ('d', 1), ('b', 1)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByKey():统计不同键的个数\n",
    "data_key.countByKey().items() \n",
    "#.items() :返回一个包含 Dictionary 对象中所有条目的数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) .saveAsTextFile():使RDD保存为文本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key.saveAsTextFile('file:///home/yanglab/lianhaimiao/PySpark/datasets/data_key.txt')\n",
    "#注：读取它需要解析文本文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) .foreach():按照一个接一个的方式，对每一条记录应用一个定义好的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "data_key.foreach(f)  #打印到CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark中的转化是惰性的，它们只在操作被调用时应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
